{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testando pyarrow e seu desempenho em relação ao python com JSON, um case iremos lidar com strings de json em uma lista, no outro case sera uma lista de dicionarios normal, em ambas, será criado um df em pyspark, pyarrow demonstra superioridade"
      ],
      "metadata": {
        "id": "Y5zcTldh9qHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspark\n",
        "#!pip install pyarrow"
      ],
      "metadata": {
        "id": "z6vm7Hnwasuf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io, json\n",
        "import pyarrow as pa\n",
        "import pyarrow.json as paj\n",
        "import pyarrow.parquet as pq\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"colab_spark_context\").getOrCreate()"
      ],
      "metadata": {
        "id": "PpTdGeObxgrj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lista de dicionarios python"
      ],
      "metadata": {
        "id": "yMDPFES_95D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(4000000):\n",
        "    data.append({\"nome\": i+1, \"idade\": i+1})\n",
        "data[0:2]\n"
      ],
      "metadata": {
        "id": "Ylh0-SNhk6dI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db8e5d6-4535-4041-9a23-91ba8d51a258"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'nome': 1, 'idade': 1}, {'nome': 2, 'idade': 2}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformando a lista de dicionarios no formato pyarrow table, levamos para parquet, pois ao que parece o pyarrow e o spark não se comunicam muito bem, mas ambos se comunicam bem com o parquet, por isso escrevemos um arquivo para acessar o df"
      ],
      "metadata": {
        "id": "g5203dv7-RJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pa.Table.from_pylist(data)\n",
        "parquet_writer = pq.ParquetWriter('output_file.parquet', x.schema)\n",
        "parquet_writer.write_table(x)\n",
        "parquet_writer.close()\n",
        "df = spark.read.parquet('output_file.parquet')\n",
        "\n",
        "#ultimo teste: 8 segundos"
      ],
      "metadata": {
        "id": "cP_auYZr9_MN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data)\n",
        "# mais de 60 segundos"
      ],
      "metadata": {
        "id": "yWOppPLj-J-q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acima vemos uma gritante diferença, mas ainda não parece usual, visto que dificilmente vemos objetos python sendo fonte (talvez para consumir API's pode ser uma boa opção). mas uma aplicação seria interessante no caso abaixo, no qual esses dicionarios python fossem uma string, tive de lidar com arquivos parquet com string json por uma medida paliativa por conta de configurações não feitas no synapse, dessa forma com certeza se tivesse um contato anterior com arrow nesse caso eu teria sido beneficiado, veja abaixo:"
      ],
      "metadata": {
        "id": "A0Pc4a8IFf6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = []\n",
        "for i in range(4000000):\n",
        "    data2.append(f'{{\"nome\": \"Nome {i}\", \"idade\": {i}}}')\n",
        "data2[0:2]\n"
      ],
      "metadata": {
        "id": "pGkhzuHs9PCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638cbef3-62c7-4f43-e774-c78f45058ec7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\"nome\": \"Nome 0\", \"idade\": 0}', '{\"nome\": \"Nome 1\", \"idade\": 1}']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformando as strings em objetos json do pyarrow, que escreve o parquet, acessando via dataframe spark"
      ],
      "metadata": {
        "id": "iLRWxO4cJt1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# o pyarrow permite que eu transforme essas strings em objetos como arquivo (file-like objects), que são acessíveis na memória, onde o pyarrow nos ajuda.\n",
        "f = io.BytesIO((\"\\n\".join(data2)).encode('utf-8'))\n",
        "\n",
        "# com esses objetos conseguimos fazer a conversão para o formato json do pyarrow, o mesmo não tem compatibilidade com spark, então para acessarmos, escrevemos um arquivo parquet e lemos posteriormente\n",
        "table = paj.read_json(f)\n",
        "\n",
        "#escrevendo arquivo\n",
        "parquet_writer = pq.ParquetWriter('output_file.parquet', table.schema)\n",
        "\n",
        "#escrevendo a tabela\n",
        "parquet_writer.write_table(table)\n",
        "\n",
        "parquet_writer.close()\n",
        "df = spark.read.parquet('output_file.parquet')\n",
        "\n",
        "#ultimo teste: 10 segundos"
      ],
      "metadata": {
        "id": "E6vD7dwTbmDQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voltando ao python, a opção que temos é ler individualmente cada string e parsea-la, o que acaba sendo muito desvantajoso em relação ao desempenho, visto que não há a possibilidade de acessar os objetos da memória."
      ],
      "metadata": {
        "id": "oeQiq8m2HX8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table2 = [json.loads(y) for y in data2]\n",
        "df2 = spark.createDataFrame(table2)\n",
        "# ultimo teste: quase dois minutos"
      ],
      "metadata": {
        "id": "da7toFwr64rz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hKJtZ1L2nHOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}